/home/s222165627/.conda/envs/domainbed/bin/python
/home/s222165627/source_code_conf/risk-distribution-matching/CMNIST
/home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/data/
Args:
	algorithm: rdm
	alpha: -10000
	batch_size: 25000
	data_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/data/
	deterministic: False
	dropout_p: 0.2
	erm_pretrain_iters: 400
	eval_freq: 50
	exp_name: reproduce
	full_resolution: False
	groupdro_eta: 1.0
	loss_fn: nll
	lr: 0.0001
	lr_cos_sched: True
	lr_factor_reduction: 1
	mlp_hidden_dim: 390
	n_workers: 0
	network: MLP
	output_dir: /home/s222165627/source_code_conf/risk-distribution-matching/CMNIST
	penalty_weight: 10000.0
	save_ckpts: True
	seed: 0
	steps: 600
	test_env_ms: 0.9
	test_envs: 0.1,0.2,0.5,0.9
	train_env_ps: (0.1, 0.2)
	train_envs: default
	variance_weight: 0.0
	weight_decay: 0
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         loss          mem_gb        step         
0.9040000000  0.3241067827  0.7952000000  0.5319377780  0.4978000000  1.0944577456  0.1017000000  1.8486604691  0.4661103964  50.000000000  0.4363019466  0.7209973335  50           
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         loss          mem_gb        step         
0.9034000000  0.3105668128  0.7953000000  0.4896648228  0.4977000000  0.9685109854  0.1028000000  1.6152442694  0.2285419846  100.00000000  0.4105432630  0.7209973335  100          
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         loss          mem_gb        step         
0.9042000000  0.3004310429  0.7949000000  0.4744654596  0.4985000000  0.9347728491  0.1036000000  1.5600467920  0.1523744424  150.00000000  0.3997321129  0.7209973335  150          
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         loss          mem_gb        step         
0.9042000000  0.2934887707  0.7953000000  0.4667946100  0.4992000000  0.9235522151  0.1050000000  1.5453463793  0.1146243548  200.00000000  0.3827092946  0.7209973335  200          
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         loss          mem_gb        step         
0.9028000000  0.2896952033  0.7941000000  0.4607104957  0.5004000000  0.9097800255  0.1096000000  1.5204402208  0.0916778679  250.00000000  0.3803153932  0.7209973335  250          
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         loss          mem_gb        step         
0.9010000000  0.2878037691  0.7938000000  0.4576509297  0.5029000000  0.9039517641  0.1154000000  1.5082635880  0.0766469534  300.00000000  0.3698323369  0.7209973335  300          
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         loss          mem_gb        step         
0.8990000000  0.2861217260  0.7936000000  0.4583293796  0.5063000000  0.9092353582  0.1243000000  1.5191737413  0.0656257268  350.00000000  0.3609808385  0.7209973335  350          
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         loss          mem_gb        step         
0.8973000000  0.2855346501  0.7926000000  0.4601066411  0.5105000000  0.9158645272  0.1316000000  1.5324281454  0.0574285054  400.00000000  0.3540177345  0.7209973335  400          
Saved ERM-pretrained model.
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         erm_loss      loss          matching_pen  mem_gb        step          variance_pen 
0.6809000000  0.6064003706  0.6824000000  0.6122686267  0.6962000000  0.6122725606  0.7195000000  0.6204779744  0.0516604524  450.00000000  0.5953474641  0.0000645847  0.0000050500  0.7212800980  450           0.2188427597 
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         erm_loss      loss          matching_pen  mem_gb        step          variance_pen 
0.7026000000  0.5811743140  0.7016000000  0.5936480165  0.7009000000  0.6122241020  0.7010000000  0.6449975371  0.0464220300  500.00000000  0.5681600571  0.0000588877  0.0000020717  0.7212800980  500           0.1979285181 
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         erm_loss      loss          matching_pen  mem_gb        step          variance_pen 
0.6998000000  0.5847786069  0.6987000000  0.5963112116  0.7004000000  0.6121508479  0.7036000000  0.6413555741  0.0419807157  550.00000000  0.5760790110  0.0000678644  0.0000102565  0.7212800980  550           0.2069361210 
0.1_acc       0.1_loss      0.2_acc       0.2_loss      0.5_acc       0.5_loss      0.9_acc       0.9_loss      avg_step_tim  epoch         erm_loss      loss          matching_pen  mem_gb        step          variance_pen 
0.7003000000  0.5843347311  0.6988000000  0.5959910750  0.7004000000  0.6121520400  0.7034000000  0.6417928338  0.0386132284  600.00000000  0.5725640059  0.0000590514  0.0000017950  0.7212800980  600           0.2029390931 

final accuracies:
0.0    0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9    1.0   
0.690  0.683  0.689  0.691  0.705  0.692  0.701  0.715  0.715  0.711  0.720 

best accuracies:
0.0    0.1    0.2    0.3    0.4    0.5    0.6    0.7    0.8    0.9    1.0   
0.663  0.663  0.671  0.677  0.699  0.688  0.702  0.719  0.725  0.724  0.738 
