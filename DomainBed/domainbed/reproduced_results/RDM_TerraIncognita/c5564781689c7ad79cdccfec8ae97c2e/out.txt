Environment:
	Python: 3.11.2
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.2
	PIL: 9.5.0
Args:
	algorithm: DGPM2
	checkpoint_freq: None
	data_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/data/
	dataset: TerraIncognita
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 2
	output_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/run_sweep/reported_test_DGPM2_TerraIncognita_manyvalues_variance/c5564781689c7ad79cdccfec8ae97c2e
	save_model_every_checkpoint: False
	seed: 1835276060
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 49
	class_balanced: False
	data_augmentation: True
	dgpm_lambda: 3.4902002767929634
	dgpm_lr: 1.9937565855330165e-05
	dgpm_penalty_anneal_iters: 836
	lr: 5.6435995676402544e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.03
	variance_weight: 0.0696917496193471
	weight_decay: 6.035203355273338e-06
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
wandb: Currently logged in as: ktoan271199 (nktoan271199). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/wandb/run-20230802_010503-3ji5f1si
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-snowball-1317
wandb:  View project at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS
wandb:  View run at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/3ji5f1si
dgpm_lambda   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         erm_loss      matching_pen  mem_gb        step          step_time     total_loss    variance_pen 
3.4902002768  0.5241233852  0.5253164557  0.0192579278  0.0231124807  0.0963476071  0.1032745592  0.2362438921  0.2465986395  0.0000000000  2.4144964218  0.1615018845  12.069724559  0             2.1636936665  2.4144964218  0.5284332633 
3.4902002768  0.8779330345  0.8691983122  0.2680703556  0.2752953261  0.7780226700  0.7556675063  0.7482472913  0.7321428571  4.6284634761  0.8468685417  0.1199843025  12.248030185  300           0.6485098600  0.8468685417  2.0200972688 
3.4902002768  0.9185341418  0.9113924051  0.3751444345  0.3898305085  0.8353274559  0.7607052897  0.8058211175  0.7831632653  9.2569269521  0.4668101946  0.0815106487  12.248030185  600           0.6466468334  0.4668101946  1.7436248620 
3.4902002768  0.9216978645  0.8966244726  0.3022210810  0.3138161274  0.8302896725  0.7846347607  0.8168684937  0.7755102041  13.885390428  0.3798218509  0.0649544207  12.248030185  900           0.6449711299  0.4148460660  1.6488894453 
3.4902002768  0.9172159241  0.8976793249  0.2420079599  0.2496147920  0.8469773300  0.7808564232  0.8132568515  0.7738095238  18.513853904  0.4007424370  0.0389623864  12.248030185  1200          0.6492712951  0.5367289685  2.0053038694 
3.4902002768  0.9201160032  0.9018987342  0.3597380922  0.3590138675  0.8630352645  0.8110831234  0.8359889526  0.7908163265  23.142317380  0.3942739344  0.0408971500  12.248030185  1500          0.6515009507  0.5370131785  1.8746333739 
3.4902002768  0.9219615080  0.8871308017  0.3674412633  0.3744221880  0.8743702771  0.8148614610  0.8455491821  0.7967687075  27.770780856  0.3723748886  0.0372477913  12.248030185  1800          0.6487579020  0.5023771398  1.8588811247 
3.4902002768  0.9272343791  0.9135021097  0.2606239569  0.2737544941  0.8699622166  0.8098236776  0.8527724665  0.8095238095  32.399244332  0.3683236732  0.0367132568  12.248030185  2100          0.6529966998  0.4964602925  1.8495057184 
3.4902002768  0.8908515687  0.8829113924  0.3634612916  0.3769902414  0.8857052897  0.8261964736  0.8480985766  0.7908163265  37.027707808  0.3309583101  0.0338397217  12.248030185  2400          0.6568825380  0.4490657167  1.6212906273 
3.4902002768  0.9274980227  0.9061181435  0.3000385159  0.3163841808  0.8535894207  0.8123425693  0.8429997876  0.8078231293  41.656171284  0.3417697551  0.0337033590  12.248030185  2700          0.6549346940  0.4594012282  1.7188666408 
3.4902002768  0.9267070920  0.9135021097  0.3999229683  0.4078068824  0.8699622166  0.7871536524  0.8629700446  0.8027210884  46.284634760  0.3557545713  0.0342721335  12.248030185  3000          0.6539439066  0.4753711822  1.8655309498 
3.4902002768  0.9090429739  0.8860759494  0.3326486070  0.3389830508  0.8948362720  0.8249370277  0.8578712556  0.8112244898  50.913098236  0.3282524018  0.0335393047  12.248030185  3300          0.6545888448  0.4453112911  1.6530494206 
3.4902002768  0.9238070129  0.8987341772  0.4734882527  0.4848484848  0.8844458438  0.8173803526  0.8708306777  0.8214285714  55.541561712  0.3411185544  0.0331836319  12.248030185  3600          0.6562570612  0.4569360759  1.7388328530 
3.4902002768  0.9098339046  0.8755274262  0.4847862370  0.4920390344  0.8920025189  0.8337531486  0.8610579987  0.7976190476  60.170025188  0.3040795754  0.0307093620  12.248030185  3900          0.6560808611  0.4112613995  1.5551610783 
3.4902002768  0.9240706565  0.8924050633  0.3968416998  0.4083204931  0.8866498741  0.8110831234  0.8568090079  0.8095238095  64.798488665  0.3129562893  0.0321115748  12.248030185  4200          0.6540396945  0.4250321175  1.6484642710 
3.4902002768  0.9338254680  0.9061181435  0.3689818975  0.3795582948  0.8853904282  0.8173803526  0.8708306777  0.8222789116  69.426952141  0.2916894567  0.0307508183  12.248030185  4500          0.6572552665  0.3990159704  1.5588122754 
3.4902002768  0.9272343791  0.9029535865  0.4383104378  0.4596815614  0.8838161209  0.8362720403  0.8710431273  0.8350340136  74.055415617  0.2846912298  0.0285484759  12.248030185  4800          0.6539751005  0.3843311281  1.6161748680 
3.4902002768  0.8990245189  0.8702531646  0.4456284504  0.4694401644  0.8942065491  0.8287153652  0.8663692373  0.8154761905  77.141057934  0.2953032434  0.0307663393  12.248030185  5000          0.6594907022  0.4026839280  1.6247088001 
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.006 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.006 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run summary:
wandb:      dgpm_lambda 3.4902
wandb:         erm_loss 0.26028
wandb: matching_penalty 0.05631
wandb:       total_loss 0.4568
wandb:     update_count 5001
wandb: variance_penalty 0.88898
wandb: 
wandb:  View run legendary-snowball-1317 at: https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/3ji5f1si
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230802_010503-3ji5f1si/logs
