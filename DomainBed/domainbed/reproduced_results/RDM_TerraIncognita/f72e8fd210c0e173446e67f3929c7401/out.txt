Environment:
	Python: 3.11.2
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.2
	PIL: 9.5.0
Args:
	algorithm: DGPM2
	checkpoint_freq: None
	data_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/data/
	dataset: TerraIncognita
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 3
	output_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/run_sweep/reported_test_DGPM2_TerraIncognita_manyvalues_variance/f72e8fd210c0e173446e67f3929c7401
	save_model_every_checkpoint: False
	seed: 784495601
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 55
	class_balanced: False
	data_augmentation: True
	dgpm_lambda: 3.453641220949037
	dgpm_lr: 1.5237924794600135e-05
	dgpm_penalty_anneal_iters: 1731
	lr: 5.991560399526455e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	variance_weight: 0.05010323853393249
	weight_decay: 5.8202860295770495e-08
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
wandb: Currently logged in as: ktoan271199 (nktoan271199). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/wandb/run-20230802_012002-8cw4n706
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-wave-1331
wandb:  View project at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS
wandb:  View run at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/8cw4n706
dgpm_lambda   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         erm_loss      matching_pen  mem_gb        step          step_time     total_loss    variance_pen 
3.4536412209  0.2035328236  0.2088607595  0.4546154834  0.4812532101  0.1832493703  0.2178841310  0.2797960484  0.2712585034  0.0000000000  2.3771140575  0.1366643906  13.537157058  0             1.8911173344  2.3771140575  0.1839095056 
3.4536412209  0.9016609544  0.8818565401  0.8247528566  0.8284540318  0.5497481108  0.5654911839  0.7590822180  0.7397959184  5.1952141058  0.7532101234  0.1863822047  13.710690498  300           0.7323678676  0.7532101234  2.0092741736 
3.4536412209  0.9182704983  0.9135021097  0.8544100655  0.8525937339  0.5390428212  0.5264483627  0.8045464202  0.7712585034  10.390428211  0.4431995647  0.0929935741  13.710690498  600           0.7317642872  0.4431995647  1.7392052408 
3.4536412209  0.9485895070  0.9229957806  0.8903581975  0.8767334361  0.5859571788  0.5869017632  0.8311026131  0.7797619048  15.585642317  0.3561695252  0.0795061111  13.710690498  900           0.7288487450  0.3561695252  1.4862987065 
3.4536412209  0.9601898234  0.9398734177  0.9044806779  0.8911145352  0.5569899244  0.5806045340  0.8440620353  0.8035714286  20.780856423  0.2979165128  0.0696739960  13.710690498  1200          0.7298720725  0.2979165128  1.2436093098 
3.4536412209  0.9615080411  0.9293248945  0.8988316857  0.8972778634  0.5620277078  0.5793450882  0.8769917145  0.8129251701  25.976070529  0.2575635626  0.0669900068  13.710690498  1500          0.7270381697  0.2575635626  1.1928040723 
3.4536412209  0.9488531505  0.9229957806  0.9206573373  0.9121725732  0.5796599496  0.5755667506  0.8859145953  0.8307823129  31.171284634  0.2320823511  0.0535396036  13.710690498  1800          0.7268607235  0.2571905831  1.1149817054 
3.4536412209  0.9754811495  0.9525316456  0.9212992682  0.9101181305  0.5692695214  0.5793450882  0.8952623752  0.8384353741  36.366498740  0.2212929208  0.0267020607  13.710690498  2100          0.7278194427  0.3135122578  1.3077541271 
3.4536412209  0.9593988927  0.9240506329  0.9175760688  0.9034411916  0.5459697733  0.5541561713  0.8914382834  0.8418367347  41.561712846  0.2169895706  0.0255546316  13.710690498  2400          0.7253097796  0.3052460982  1.2823652687 
3.4536412209  0.9728447139  0.9440928270  0.9265631018  0.9162814587  0.5752518892  0.5944584383  0.9067346505  0.8503401361  46.756926952  0.2021016609  0.0233173625  13.710690498  2700          0.7287634683  0.2826314637  1.2029131126 
3.4536412209  0.9733720011  0.9462025316  0.9250224676  0.9070364664  0.5513224181  0.5617128463  0.9001487147  0.8307823129  51.952141057  0.2017023944  0.0217715073  13.710690498  3000          0.6999279976  0.2768933692  1.2059746596 
3.4536412209  0.9717901397  0.9451476793  0.9295159841  0.9116589625  0.5481738035  0.5554156171  0.9133205864  0.8503401361  57.147355163  0.1896032029  0.0215077114  13.710690498  3300          0.7019448241  0.2638831203  1.1829259470 
3.4536412209  0.9749538624  0.9493670886  0.9297727565  0.9183359014  0.5469143577  0.5680100756  0.9162948800  0.8630952381  62.342569269  0.1878271743  0.0202447097  13.710690498  3600          0.7307954510  0.2577451371  1.1708496014 
3.4536412209  0.9686264171  0.9398734177  0.9363204519  0.9234720082  0.5733627204  0.5806045340  0.9086466964  0.8520408163  67.537783375  0.1766160479  0.0201256243  13.710690498  3900          0.7290629133  0.2461227338  1.0632347494 
3.4536412209  0.9657263380  0.9462025316  0.9395301066  0.9142270159  0.5790302267  0.5881612091  0.9284045039  0.8622448980  72.732997481  0.1729059437  0.0202208296  13.710690498  4200          0.7287399777  0.2427414334  1.1271555843 
3.4536412209  0.9762720801  0.9472573840  0.9347798177  0.9157678480  0.5916246851  0.6095717884  0.9116209900  0.8460884354  77.928211586  0.1774730361  0.0201458995  13.710690498  4500          0.7301681614  0.2470497436  1.1520886377 
3.4536412209  0.9699446349  0.9451476793  0.9320837078  0.9162814587  0.5251889169  0.5302267003  0.9111960909  0.8537414966  83.123425692  0.1733613348  0.0201105976  13.710690498  4800          0.7261835273  0.2428161230  1.0984371679 
3.4536412209  0.9733720011  0.9419831224  0.9223263577  0.9049820236  0.5680100756  0.5806045340  0.9203314213  0.8639455782  86.586901763  0.1699010337  0.0184775257  13.710690498  5000          0.7282899296  0.2337157771  1.0554526734 
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run summary:
wandb:      dgpm_lambda 3.45364
wandb:         erm_loss 0.16315
wandb: matching_penalty 0.01087
wandb:       total_loss 0.20069
wandb:     update_count 5001
wandb: variance_penalty 0.71946
wandb: 
wandb:  View run noble-wave-1331 at: https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/8cw4n706
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230802_012002-8cw4n706/logs
