Environment:
	Python: 3.11.2
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.2
	PIL: 9.5.0
Args:
	algorithm: DGPM2
	checkpoint_freq: None
	data_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/data/
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 1
	output_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/run_sweep/reported_test3Aug_DGPM2_OfficeHome_lowerweight_matching/7583dfe2886ab1739bdac7f3f9827a42
	save_model_every_checkpoint: False
	seed: 517519199
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 96
	class_balanced: False
	data_augmentation: True
	dgpm_lambda: 0.1518462950381929
	dgpm_lr: 8.266310922728087e-06
	dgpm_penalty_anneal_iters: 2287
	lr: 4.4049700015373634e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	variance_weight: 0.0
	weight_decay: 9.743107780992378e-07
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
wandb: Currently logged in as: ktoan271199 (nktoan271199). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.8 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/wandb/run-20230803_154736-spy95cfd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-smoke-1541
wandb:  View project at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS
wandb:  View run at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/spy95cfd
dgpm_lambda   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         erm_loss      matching_pen  mem_gb        step          step_time     total_loss    variance_pen 
0.1518462950  0.0242018538  0.0123711340  0.0131729668  0.0183276060  0.0171734234  0.0236753100  0.0100401606  0.0137772675  0.0000000000  4.2389574051  0.0111379623  23.549080848  0             2.3532347679  4.2389574051  0.2148883939 
0.1518462950  0.9691040165  0.7175257732  0.9180985109  0.7342497136  0.7401463964  0.7361894025  0.9454962708  0.8094144661  14.830072090  0.7142431352  0.0351025740  23.723642349  300           1.4432108339  0.7142431352  2.6246658626 
0.1518462950  0.9824922760  0.7319587629  0.9550400916  0.7548682703  0.7328265766  0.7328072153  0.9733218589  0.7933409874  29.660144181  0.1168759172  0.0172897847  23.723642349  600           1.4427711884  0.1168759172  0.7004946168 
0.1518462950  0.9902162719  0.7298969072  0.9753722795  0.7709049255  0.7559121622  0.7328072153  0.9885255307  0.8025258324  44.490216271  0.0664114974  0.0111846352  23.723642349  900           1.4696143174  0.0664114974  0.4522280299 
0.1518462950  0.9922760041  0.7422680412  0.9782359679  0.7491408935  0.7550675676  0.7384441939  0.9873780838  0.8071182549  59.320288362  0.0489713641  0.0082810688  23.723642349  1200          1.4464493839  0.0489713641  0.3566993813 
0.1518462950  0.9917610711  0.7587628866  0.9768041237  0.7502863688  0.7533783784  0.7452085682  0.9908204246  0.8094144661  74.150360453  0.0420701797  0.0069336637  23.723642349  1500          1.4396877297  0.0420701797  0.2715355279 
0.1518462950  0.9933058702  0.7484536082  0.9747995418  0.7731958763  0.7553490991  0.7418263811  0.9899598394  0.8048220436  88.980432543  0.0381322989  0.0057864189  23.723642349  1800          1.4573920266  0.0381322989  0.2606415193 
0.1518462950  0.9922760041  0.7525773196  0.9805269187  0.7628865979  0.7308558559  0.7339346110  0.9882386690  0.8094144661  103.81050463  0.0372899919  0.0055017440  23.723642349  2100          1.4509788799  0.0372899919  0.2526901033 
0.1518462950  0.9963954686  0.7793814433  0.9856815578  0.7880870561  0.7657657658  0.7733934611  0.9971313827  0.8323765786  118.64057672  0.0334605019  0.0048776340  23.723642349  2400          1.4297997061  0.0336408912  0.2372540020 
0.1518462950  0.9953656025  0.7731958763  0.9862542955  0.7949599084  0.7576013514  0.7395715896  0.9968445209  0.8231917336  133.47064881  0.0157091807  0.0026449966  23.723642349  2700          1.4297712755  0.0161108137  0.0837040509 
0.1518462950  0.9963954686  0.7525773196  0.9873997709  0.7961053837  0.7587274775  0.7406989853  0.9968445209  0.8220436280  148.30072090  0.0143350459  0.0023582554  23.723642349  3000          1.4457532899  0.0146931382  0.0840459146 
0.1518462950  0.9958805355  0.8020618557  0.9856815578  0.7926689576  0.7697072072  0.7666290868  0.9971313827  0.8381171068  163.13079299  0.0145631072  0.0025542132  23.723642349  3300          1.4549367404  0.0149509550  0.0794232767 
0.1518462950  0.9974253347  0.7752577320  0.9871134021  0.7903780069  0.7567567568  0.7542277339  0.9965576592  0.8231917336  177.96086508  0.0129312192  0.0020845350  23.723642349  3600          1.4479767394  0.0132477480  0.0636822973 
0.1518462950  0.9963954686  0.7587628866  0.9879725086  0.7835051546  0.7502815315  0.7406989853  0.9968445209  0.8243398393  192.79093717  0.0133752242  0.0020986334  23.723642349  3900          1.4408819477  0.0136938939  0.0677751070 
0.1518462950  0.9958805355  0.7505154639  0.9885452463  0.7961053837  0.7505630631  0.7429537768  0.9959839357  0.8289322618  207.62100926  0.0121636135  0.0019772879  23.723642349  4200          1.4422247020  0.0124638574  0.0555702732 
0.1518462950  0.9969104016  0.7752577320  0.9873997709  0.7697594502  0.7463400901  0.7395715896  0.9968445209  0.8335246843  222.45108135  0.0124162770  0.0019728216  23.723642349  4500          1.4457853508  0.0127158426  0.0535672330 
0.1518462950  0.9953656025  0.7876288660  0.9888316151  0.7800687285  0.7697072072  0.7632468997  0.9962707975  0.8289322618  237.28115345  0.0124858318  0.0018460210  23.723642349  4800          1.4535060684  0.0127661433  0.0651117696 
0.1518462950  0.9958805355  0.7835051546  0.9873997709  0.7800687285  0.7646396396  0.7621195039  0.9974182444  0.8140068886  247.16786817  0.0120632169  0.0018019676  23.723642349  5000          1.4641409492  0.0123368390  0.0511238280 
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.006 MB of 0.006 MB uploaded (0.000 MB deduped)wandb: \ 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: | 0.008 MB of 0.014 MB uploaded (0.000 MB deduped)wandb: 
wandb: Run summary:
wandb:      dgpm_lambda 0.15185
wandb:         erm_loss 0.02242
wandb: matching_penalty 0.00152
wandb:       total_loss 0.02265
wandb:     update_count 5001
wandb: variance_penalty 0.14893
wandb: 
wandb:  View run amber-smoke-1541 at: https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/spy95cfd
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230803_154736-spy95cfd/logs
