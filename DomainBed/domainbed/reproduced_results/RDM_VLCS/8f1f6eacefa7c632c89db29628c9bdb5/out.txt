Environment:
	Python: 3.11.2
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.2
	PIL: 9.5.0
Args:
	algorithm: DGPM2
	checkpoint_freq: None
	data_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 3
	output_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/run_sweep/reported_final2_VLCS/8f1f6eacefa7c632c89db29628c9bdb5
	save_model_every_checkpoint: False
	seed: 1099162840
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 1
	uda_holdout_fraction: 0
HParams:
	batch_size: 65
	class_balanced: False
	data_augmentation: True
	dgpm_lambda: 9.140272822495048
	dgpm_lr: 1.0412618729905338e-05
	dgpm_penalty_anneal_iters: 1801
	lr: 9.100567295205603e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.003
	variance_weight: 0.0035079968512577832
	weight_decay: 2.2710283595807042e-07
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 3, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
wandb: Currently logged in as: ktoan271199 (nktoan271199). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/wandb/run-20230729_030007-pr6xxdee
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-pond-647
wandb:  View project at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS
wandb:  View run at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/pr6xxdee
dgpm_lambda   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         erm_loss      matching_pen  mem_gb        step          step_time     total_loss    variance_pen 
9.1402728225  0.6139575972  0.6183745583  0.4743529412  0.4632768362  0.3762376238  0.4192073171  0.4513143280  0.4088888889  0.0000000000  1.6499652863  0.3202610016  15.986212730  0             4.4872627258  1.6499652863  0.1353364140 
9.1402728225  0.9929328622  0.9893992933  0.8225882353  0.7118644068  0.8678598629  0.7576219512  0.6827101074  0.6800000000  17.226148409  0.3617154987  0.2796376260  16.159702301  300           1.5935343107  0.3617154987  1.3308208314 
9.1402728225  1.0000000000  0.9823321555  0.9134117647  0.7269303202  0.9657273420  0.8262195122  0.7630507220  0.7437037037  34.452296819  0.1779803631  0.1888687833  16.159702301  600           1.6068482351  0.1779803631  0.8430265974 
9.1402728225  1.0000000000  0.9787985866  0.9717647059  0.7419962335  0.9722010663  0.7713414634  0.7404664939  0.7303703704  51.678445229  0.0987805243  0.0788064130  16.159702301  900           1.6153192663  0.0987805243  0.5643710685 
9.1402728225  1.0000000000  0.9858657244  0.9807058824  0.7288135593  0.9878141660  0.8079268293  0.7138097001  0.7051851852  68.904593639  0.0626367009  0.0390877151  16.159702301  1200          1.6086467600  0.0626367009  0.4378092951 
9.1402728225  0.9946996466  0.9893992933  0.9905882353  0.7495291902  0.9851485149  0.7820121951  0.7126990004  0.6874074074  86.130742049  0.0479693372  0.0232124233  16.159702301  1500          1.6235717519  0.0479693372  0.3780985443 
9.1402728225  1.0000000000  0.9893992933  0.9887058824  0.7363465160  0.9927646611  0.7972560976  0.7130692336  0.7200000000  103.35689045  0.0298433788  0.0137297217  16.159702301  1800          1.6158484348  0.0298433788  0.2391190830 
9.1402728225  1.0000000000  1.0000000000  0.9971764706  0.7514124294  0.9988575781  0.7835365854  0.7008515365  0.6800000000  120.58303886  0.0055574950  0.0011580785  16.159702301  2100          1.6133873423  0.0163186107  0.0501601903 
9.1402728225  1.0000000000  0.9858657244  0.9962352941  0.7476459510  0.9977151561  0.7881097561  0.7101073676  0.6903703704  137.80918727  0.0038616548  0.0005140241  16.159702301  2400          1.6237723907  0.0088404679  0.0799580048 
9.1402728225  1.0000000000  0.9929328622  0.9985882353  0.7193973635  1.0000000000  0.8018292683  0.7219548315  0.7051851852  155.03533568  0.0029060765  0.0003948911  16.159702301  2700          1.6269109074  0.0066836239  0.0479290380 
9.1402728225  1.0000000000  0.9929328622  1.0000000000  0.7288135593  0.9992383854  0.7637195122  0.7019622362  0.7007407407  172.26148409  0.0011869796  0.0002081108  16.159702301  3000          1.6219089699  0.0031428841  0.0153121075 
9.1402728225  1.0000000000  0.9964664311  1.0000000000  0.7532956685  0.9980959634  0.7743902439  0.7071455017  0.6977777778  189.48763250  0.0029928972  0.0003842926  16.159702301  3300          1.6139765867  0.0066593965  0.0438882919 
9.1402728225  1.0000000000  0.9893992933  0.9990588235  0.7363465160  0.9992383854  0.8094512195  0.7234357645  0.7200000000  206.71378091  0.0017230632  0.0002161884  16.159702301  3600          1.6193802460  0.0037932869  0.0268536043 
9.1402728225  1.0000000000  0.9929328622  0.9985882353  0.7457627119  0.9988575781  0.7820121951  0.7075157349  0.6888888889  223.93992932  0.0027260473  0.0003396320  16.159702301  3900          1.6154709967  0.0060099000  0.0511754176 
9.1402728225  1.0000000000  0.9929328622  0.9990588235  0.7401129944  0.9992383854  0.7957317073  0.7212143650  0.7140740741  241.16607773  0.0017065876  0.0002205690  16.159702301  4200          1.6089225650  0.0038198070  0.0276963360 
9.1402728225  1.0000000000  0.9964664311  0.9995294118  0.7514124294  0.9988575781  0.7972560976  0.7167715661  0.7185185185  258.39222614  0.0026611349  0.0003082879  16.159702301  4500          1.6263752786  0.0056366495  0.0449483511 
9.1402728225  1.0000000000  0.9964664311  0.9990588235  0.7476459510  0.9996191927  0.7972560976  0.7378748612  0.7170370370  275.61837455  0.0025656071  0.0002978802  16.159702301  4800          1.6146256638  0.0054850309  0.0560769293 
9.1402728225  1.0000000000  1.0000000000  0.9990588235  0.7419962335  0.9996191927  0.7926829268  0.7152906331  0.7155555556  287.10247349  0.0011775572  0.0001630306  16.159702301  5000          1.5997713244  0.0027245412  0.0162028666 
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run summary:
wandb:      dgpm_lambda 9.14027
wandb:         erm_loss 0.0
wandb: matching_penalty 0.0
wandb:       total_loss 0.0
wandb:     update_count 5001
wandb: variance_penalty 0.0
wandb: 
wandb:  View run generous-pond-647 at: https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/pr6xxdee
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230729_030007-pr6xxdee/logs
