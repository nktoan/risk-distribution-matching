Environment:
	Python: 3.11.2
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.2
	PIL: 9.5.0
Args:
	algorithm: DGPM2
	checkpoint_freq: None
	data_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/data/
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 1
	output_dir: /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/domainbed/run_sweep/reported_final2_VLCS/e47c17a25c4fc6d271d25b775eef212f
	save_model_every_checkpoint: False
	seed: 818729373
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 96
	class_balanced: False
	data_augmentation: True
	dgpm_lambda: 1.5184629503819291
	dgpm_lr: 8.266310922728087e-06
	dgpm_penalty_anneal_iters: 2287
	lr: 4.4049700015373634e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	variance_weight: 0.004685630281242425
	weight_decay: 9.743107780992378e-07
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/s222165627/.conda/envs/domainbed/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
wandb: Currently logged in as: ktoan271199 (nktoan271199). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.5
wandb: Run data is saved locally in /home/s222165627/causal_optimisation_dg/quantile_rm/DomainBed/wandb/run-20230729_021410-9bsstxx4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-cherry-621
wandb:  View project at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS
wandb:  View run at https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/9bsstxx4
dgpm_lambda   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         erm_loss      matching_pen  mem_gb        step          step_time     total_loss    variance_pen 
1.5184629504  0.6121908127  0.6289752650  0.4597647059  0.4896421846  0.3842345773  0.3871951220  0.4435394298  0.4459259259  0.0000000000  1.6406183243  0.0537605286  23.548100948  0             21.425435304  1.6406183243  0.1715186238 
1.5184629504  1.0000000000  1.0000000000  0.9129411765  0.7589453861  0.7269611577  0.6981707317  0.9692706405  0.8177777778  25.441696113  0.2624138433  0.3293828583  23.721746921  300           3.5788865177  0.2624138433  1.0190877245 
1.5184629504  1.0000000000  0.9964664311  0.9694117647  0.7645951036  0.6873571973  0.6585365854  0.9859311366  0.8340740741  50.883392226  0.0792749752  0.0925246175  23.721746921  600           3.6211056614  0.0792749752  0.3770201079 
1.5184629504  1.0000000000  0.9964664311  0.9868235294  0.7721280603  0.7292460015  0.7057926829  0.9911144021  0.8281481481  76.325088339  0.0272090522  0.0148603344  23.721746921  900           3.6565564330  0.0272090522  0.1829119005 
1.5184629504  1.0000000000  0.9929328622  0.9971764706  0.7777777778  0.7102056359  0.6844512195  0.9977786005  0.8222222222  101.76678445  0.0232901144  0.0093364016  23.721746921  1200          3.6475194446  0.0232901144  0.1599806594 
1.5184629504  1.0000000000  1.0000000000  0.9872941176  0.7363465160  0.6759329779  0.6722560976  0.9859311366  0.7911111111  127.20848056  0.0136277551  0.0044641018  23.721746921  1500          3.6398846753  0.0136277551  0.0928840531 
1.5184629504  1.0000000000  1.0000000000  0.9971764706  0.7570621469  0.7269611577  0.7149390244  0.9996297668  0.8444444444  152.65017667  0.0133554449  0.0045184835  23.721746921  1800          3.6029004502  0.0133554449  0.0970858856 
1.5184629504  1.0000000000  0.9893992933  0.9981176471  0.7627118644  0.7075399848  0.6798780488  0.9981488338  0.8162962963  178.09187279  0.0124995479  0.0039274279  23.721746921  2100          3.3535194643  0.0124995479  0.1008307569 
1.5184629504  1.0000000000  0.9964664311  0.9995294118  0.7608286252  0.7033511043  0.6905487805  0.9985190670  0.8444444444  203.53356890  0.0076803682  0.0020998510  23.721746921  2400          3.3767862638  0.0080834308  0.0659026366 
1.5184629504  1.0000000000  0.9964664311  0.9981176471  0.7401129944  0.7174409749  0.6951219512  1.0000000000  0.8325925926  228.97526501  0.0014007475  0.0002106444  23.721746921  2700          3.3440454507  0.0018038979  0.0177766144 
1.5184629504  1.0000000000  1.0000000000  0.9990588235  0.7796610169  0.6835491241  0.6737804878  0.9992595335  0.8400000000  254.41696113  0.0012227746  0.0001886272  23.721746921  3000          3.3532273157  0.0015566964  0.0101370316 
1.5184629504  1.0000000000  0.9964664311  0.9995294118  0.7551789077  0.7174409749  0.6890243902  0.9996297668  0.8459259259  279.85865724  0.0014228689  0.0002147230  23.721746921  3300          3.3568281953  0.0018153225  0.0141720002 
1.5184629504  1.0000000000  1.0000000000  0.9985882353  0.7457627119  0.7185833968  0.6951219512  0.9996297668  0.8459259259  305.30035335  0.0008836867  0.0001463318  23.721746921  3600          3.3351608062  0.0011364151  0.0065154511 
1.5184629504  1.0000000000  1.0000000000  1.0000000000  0.7514124294  0.7121096725  0.6783536585  1.0000000000  0.8340740741  330.74204947  0.0013764294  0.0001531665  23.721746921  3900          3.3328878585  0.0017180760  0.0232773327 
1.5184629504  1.0000000000  0.9964664311  1.0000000000  0.7796610169  0.7022086824  0.6859756098  0.9992595335  0.8370370370  356.18374558  0.0007804764  0.0001327674  23.721746921  4200          3.3526579563  0.0010145390  0.0069276320 
1.5184629504  1.0000000000  0.9964664311  1.0000000000  0.7740112994  0.6968773800  0.6783536585  1.0000000000  0.8414814815  381.62544169  0.0013273091  0.0001922003  23.721746921  4500          3.3507363510  0.0016861971  0.0143073518 
1.5184629504  1.0000000000  1.0000000000  0.9990588235  0.7721280603  0.7174409749  0.6890243902  1.0000000000  0.8385185185  407.06713780  0.0009931026  0.0001492691  23.721746921  4800          3.3219833779  0.0012577006  0.0080967585 
1.5184629504  1.0000000000  1.0000000000  1.0000000000  0.7532956685  0.7220106626  0.6859756098  1.0000000000  0.8311111111  424.02826855  0.0011994719  0.0001594496  23.721746921  5000          3.3422170162  0.0015118301  0.0149905048 
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run summary:
wandb:      dgpm_lambda 1.51846
wandb:         erm_loss 7e-05
wandb: matching_penalty 3e-05
wandb:       total_loss 0.00011
wandb:     update_count 5001
wandb: variance_penalty 0.0
wandb: 
wandb:  View run helpful-cherry-621 at: https://wandb.ai/nktoan271199/risk_distribution_matching_PACS/runs/9bsstxx4
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20230729_021410-9bsstxx4/logs
